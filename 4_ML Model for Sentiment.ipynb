{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code 3-29 to Code 3-47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "import warnings\n",
    "import stop_words\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.read_csv(\"lexicon_sent_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = t1.loc[:,\"score_bkt\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML model for sentiment contains 2 sets of features\n",
    "1. Text Features\n",
    "We want to create a generic sentiment analysis module - we select only words that are relevant to emotions or sentiment from the corpus. This is an important step as using all the words  (in our case pet food names, candy names etc) could bias the model to learn patterns that associate product and sentiment. We create sets of word features - positive/negative words, adjectives and selecting only “stop words”. \n",
    "\n",
    "2. Numeric Features\n",
    "Lexical features created so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get list of words to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnv_str(x):\n",
    "    x1 = list(eval(x))\n",
    "    x2 = ' '.join(x1)\n",
    "    return x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get Adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pos(fltr,sent_list):\n",
    "    str1 = \"\"\n",
    "    for i in sent_list:\n",
    "        if(i[1]==\"JJ\"):\n",
    "            str1 = str1 + i[0].lower() + \" \"\n",
    "    return str1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get stop words in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stop_words(sent):\n",
    "    list1 = set(sent.split())\n",
    "    st_comm = list(list1 & st_set)\n",
    "    st_comm = ' '.join(st_comm)\n",
    "    return st_comm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the text corpus from \n",
    "1. positive words, negative words identified (using lexicons)\n",
    "2. Stop words in that sentence\n",
    "3. Adjectives found in that sentence\n",
    "\n",
    "These will be treated as text features in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[\"pos_set1\"] = t1[\"pos_set\"].apply(cnv_str)\n",
    "t1[\"neg_set1\"] = t1[\"neg_set\"].apply(cnv_str)\n",
    "t1[\"pos_neg_comb\"] = t1[\"pos_set1\"] + \" \" + t1[\"neg_set1\"]\n",
    "\n",
    "get_pos_tags = nltk.pos_tag_sents(t1[\"Text\"].str.split())\n",
    "\n",
    "str_sel_list = []\n",
    "for i in get_pos_tags:\n",
    "    str_sel = filter_pos(\"JJ\",i)\n",
    "    str_sel_list.append(str_sel)\n",
    "    \n",
    "t1[\"pos_neg_comb_adj\"] = t1[\"pos_neg_comb\"] + str_sel_list\n",
    "\n",
    "st1 = stop_words.get_stop_words('en')\n",
    "st_set = set(st1)\n",
    "onl_stop_words = t1[\"full_txt\"].apply(get_stop_words)\n",
    "\n",
    "t1[\"pos_neg_comb_adj_st\"] = t1[\"pos_neg_comb_adj\"] + onl_stop_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(test_size=0.8,random_state=42,n_splits=1)\n",
    "\n",
    "for train_index, test_index in sss.split(t1, tgt):\n",
    "    x_train, x_test = t1[t1.index.isin(train_index)], t1[t1.index.isin(test_index)]\n",
    "    y_train, y_test = t1.loc[t1.index.isin(train_index),\"score_bkt\"], t1.loc[t1.index.isin(test_index),\"score_bkt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Variables created in \"1_Lexicon based Sentiment_ver2_vader.ipynb\" file and the text corpus created in line 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inde_vars = [\"sent_len\",\"pos_score\",\"neg_score\",\"neg_num_pos_count\",\"neg_num_neg_count\", 'boost_num_pos_count', 'boost_num_neg_count', 'neg_num_pos_count_sum',\n",
    "       'neg_num_neg_count_sum', 'boost_num_pos_count_sum',\n",
    "       'boost_num_neg_count_sum', 'excl_num_pos_count', 'excl_num_neg_count',\n",
    "       'excl_num_pos_count_sum', 'excl_num_neg_count_sum'\n",
    "            ]\n",
    "x_train1 = x_train[inde_vars]\n",
    "x_test1 = x_test[inde_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_len</th>\n",
       "      <th>pos_score</th>\n",
       "      <th>neg_score</th>\n",
       "      <th>neg_num_pos_count</th>\n",
       "      <th>neg_num_neg_count</th>\n",
       "      <th>boost_num_pos_count</th>\n",
       "      <th>boost_num_neg_count</th>\n",
       "      <th>neg_num_pos_count_sum</th>\n",
       "      <th>neg_num_neg_count_sum</th>\n",
       "      <th>boost_num_pos_count_sum</th>\n",
       "      <th>boost_num_neg_count_sum</th>\n",
       "      <th>excl_num_pos_count</th>\n",
       "      <th>excl_num_neg_count</th>\n",
       "      <th>excl_num_pos_count_sum</th>\n",
       "      <th>excl_num_neg_count_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>111.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>38.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_len  pos_score  neg_score  neg_num_pos_count  neg_num_neg_count  \\\n",
       "5       49.0        3.5        8.0                  0                  0   \n",
       "6       86.0        2.0        4.0                  0                  0   \n",
       "8      111.0        8.0        0.0                  0                  0   \n",
       "49      38.0        7.5        1.0                  0                  0   \n",
       "51      24.0        3.0        0.0                  0                  0   \n",
       "\n",
       "    boost_num_pos_count  boost_num_neg_count  neg_num_pos_count_sum  \\\n",
       "5                     0                    1                      0   \n",
       "6                     0                    0                      0   \n",
       "8                     0                    0                      0   \n",
       "49                    0                    0                      0   \n",
       "51                    1                    0                      0   \n",
       "\n",
       "    neg_num_neg_count_sum  boost_num_pos_count_sum  boost_num_neg_count_sum  \\\n",
       "5                       0                        0                        1   \n",
       "6                       0                        0                        0   \n",
       "8                       0                        0                        0   \n",
       "49                      0                        0                        0   \n",
       "51                      0                        0                        0   \n",
       "\n",
       "    excl_num_pos_count  excl_num_neg_count  excl_num_pos_count_sum  \\\n",
       "5                    0                   0                       0   \n",
       "6                    0                   0                       0   \n",
       "8                    1                   0                       0   \n",
       "49                   1                   0                       0   \n",
       "51                   0                   0                       0   \n",
       "\n",
       "    excl_num_neg_count_sum  \n",
       "5                        0  \n",
       "6                        0  \n",
       "8                        0  \n",
       "49                       0  \n",
       "51                       0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Applying TFIDF for text feature created in line 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=0.001,analyzer=u'word',ngram_range=(1,1))\n",
    "tfidf_matrix_tr = tfidf_vectorizer.fit_transform(x_train[\"pos_neg_comb_adj_st\"])\n",
    "\n",
    "tfidf_matrix_te = tfidf_vectorizer.transform(x_test[\"pos_neg_comb_adj_st\"])\n",
    "\n",
    "x_train2= tfidf_matrix_tr.todense()\n",
    "x_test2 = tfidf_matrix_te.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combining numeric feature and text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x_train3 = np.concatenate([x_train1,x_train2],axis=1)\n",
    "x_test3 = np.concatenate([x_test1,x_test2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((11368, 1194), (45475, 1194))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train3.shape, x_test3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Scaling features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_train_scaled = min_max_scaler.fit_transform(x_train3)\n",
    "x_test_scaled = min_max_scaler.transform(x_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "selector = SelectPercentile(f_classif, percentile=40)\n",
    "selector.fit(x_train3,y_train)\n",
    "x_train4 = selector.fit_transform(x_train_scaled,y_train)\n",
    "x_test4 = selector.transform(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11368, 478)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Dependent variables\n",
    "We also need to convert the categorical target variable into an encoded into one hot encoding form. \n",
    "One hot encoding is a form where each level is represented by the absence of the other levels and the presence of that level. \n",
    "For eg Positive can be represented as 100,negative 010 and neutral 001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "le = LabelEncoder()\n",
    "y_train1 = le.fit_transform(y_train)\n",
    "y_train2 = np_utils.to_categorical(y_train1)\n",
    "y_test1 = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11368, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Building neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input, Dense, Dropout\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_mod(list_layers,dp):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(list_layers[0], input_dim=x_train4.shape[1], activation='tanh', kernel_initializer='lecun_uniform'))\n",
    "    model.add(Dropout(dp))\n",
    "\n",
    "    for i in list_layers[1:]:\n",
    "        model.add(Dense(i, input_dim=x_train4.shape[1], activation='tanh'))\n",
    "        model.add(Dropout(dp))\n",
    "        \n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    opt = Adam(0.0001)\n",
    "# Compile model\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "list_layers = [500,200,100,50]\n",
    "class_weight = {0:0.2,1:0.6,2:0.2}\n",
    "\n",
    "model = get_nn_mod(list_layers,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9094 samples, validate on 2274 samples\n",
      "Epoch 1/20\n",
      " - 20s - loss: 0.1863 - acc: 0.7646 - val_loss: 0.1622 - val_acc: 0.7814\n",
      "Epoch 2/20\n",
      " - 1s - loss: 0.1450 - acc: 0.8023 - val_loss: 0.1352 - val_acc: 0.8012\n",
      "Epoch 3/20\n",
      " - 1s - loss: 0.1286 - acc: 0.8223 - val_loss: 0.1288 - val_acc: 0.8127\n",
      "Epoch 4/20\n",
      " - 1s - loss: 0.1214 - acc: 0.8302 - val_loss: 0.1271 - val_acc: 0.8113\n",
      "Epoch 5/20\n",
      " - 1s - loss: 0.1165 - acc: 0.8386 - val_loss: 0.1290 - val_acc: 0.8329\n",
      "Epoch 6/20\n",
      " - 1s - loss: 0.1136 - acc: 0.8431 - val_loss: 0.1278 - val_acc: 0.8228\n",
      "Epoch 7/20\n",
      " - 1s - loss: 0.1104 - acc: 0.8468 - val_loss: 0.1284 - val_acc: 0.8232\n",
      "Epoch 8/20\n",
      " - 1s - loss: 0.1097 - acc: 0.8457 - val_loss: 0.1295 - val_acc: 0.8232\n",
      "Epoch 9/20\n",
      " - 1s - loss: 0.1075 - acc: 0.8500 - val_loss: 0.1317 - val_acc: 0.8281\n",
      "Epoch 10/20\n",
      " - 1s - loss: 0.1065 - acc: 0.8479 - val_loss: 0.1335 - val_acc: 0.8166\n",
      "Epoch 11/20\n",
      " - 1s - loss: 0.1063 - acc: 0.8527 - val_loss: 0.1311 - val_acc: 0.8118\n",
      "Epoch 12/20\n",
      " - 1s - loss: 0.1048 - acc: 0.8514 - val_loss: 0.1336 - val_acc: 0.8127\n",
      "Epoch 13/20\n",
      " - 1s - loss: 0.1049 - acc: 0.8540 - val_loss: 0.1345 - val_acc: 0.8162\n",
      "Epoch 14/20\n",
      " - 1s - loss: 0.1046 - acc: 0.8513 - val_loss: 0.1333 - val_acc: 0.8144\n",
      "Epoch 15/20\n",
      " - 1s - loss: 0.1041 - acc: 0.8516 - val_loss: 0.1347 - val_acc: 0.8105\n",
      "Epoch 16/20\n",
      " - 1s - loss: 0.1037 - acc: 0.8533 - val_loss: 0.1350 - val_acc: 0.8135\n",
      "Epoch 17/20\n",
      " - 1s - loss: 0.1035 - acc: 0.8508 - val_loss: 0.1386 - val_acc: 0.8219\n",
      "Epoch 18/20\n",
      " - 1s - loss: 0.1041 - acc: 0.8535 - val_loss: 0.1357 - val_acc: 0.8113\n",
      "Epoch 19/20\n",
      " - 1s - loss: 0.1027 - acc: 0.8566 - val_loss: 0.1359 - val_acc: 0.8065\n",
      "Epoch 20/20\n",
      " - 1s - loss: 0.1029 - acc: 0.8558 - val_loss: 0.1366 - val_acc: 0.8118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1539ab00>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train4,y_train2, batch_size=100, epochs=20,class_weight=class_weight,\n",
    "          verbose=2,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Measuring model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069488730071468 0.5860706406689715\n"
     ]
    }
   ],
   "source": [
    "pred=model.predict_classes(x_test4)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "ac1 = accuracy_score(y_test1, pred)\n",
    "print (ac1, f1_score(y_test1, pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neu</th>\n",
       "      <th>neg</th>\n",
       "      <th>act</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32422</td>\n",
       "      <td>2291</td>\n",
       "      <td>867</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1649</td>\n",
       "      <td>1234</td>\n",
       "      <td>502</td>\n",
       "      <td>neu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2082</td>\n",
       "      <td>1388</td>\n",
       "      <td>3040</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pos   neu   neg  act\n",
       "0  32422  2291   867  pos\n",
       "1   1649  1234   502  neu\n",
       "2   2082  1388  3040  neg"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "rows_name = t1[\"score_bkt\"].unique()\n",
    "pred_inv = le.inverse_transform(pred)\n",
    "\n",
    "cmat = pd.DataFrame(confusion_matrix(y_test, pred_inv, labels=rows_name, sample_weight=None))\n",
    "cmat.columns = rows_name \n",
    "cmat[\"act\"] = rows_name\n",
    "cmat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda2-py36]",
   "language": "python",
   "name": "conda-env-Anaconda2-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
